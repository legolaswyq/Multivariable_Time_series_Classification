{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use pytorch Dataset and Dataloader \n",
    "1. Dataset need to implement the __len__() and __getitem__ function \n",
    "2. Dataloader, specify the batch size and shuffle \n",
    "3. training loop \n",
    "4. loss function \n",
    "5. prediction \n",
    "\n",
    "torch.utils.data.TensorDataset is a convenient class in PyTorch that allows you to create a dataset from one or more tensors. It is useful when your data is already in the form of tensors and you want to quickly create a dataset without writing a custom Dataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/sensor/train.csv')\n",
    "labels = pd.read_csv('../data/sensor/train_labels.csv')\n",
    "test = pd.read_csv('../data/sensor/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sequence  subject  step  sensor_00  sensor_01  sensor_02  sensor_03  \\\n",
      "0         0       47     0  -0.196291   0.112395        1.0   0.329204   \n",
      "1         0       47     1  -0.447450   0.134454        1.0  -0.658407   \n",
      "2         0       47     2   0.326893  -0.694328        1.0   0.330088   \n",
      "3         0       47     3   0.523184   0.751050        1.0   0.976991   \n",
      "4         0       47     4   0.272025   1.074580        1.0  -0.136283   \n",
      "\n",
      "   sensor_04  sensor_05  sensor_06  sensor_07  sensor_08  sensor_09  \\\n",
      "0  -1.004660  -0.131638  -0.127505   0.368702       -0.1  -0.963873   \n",
      "1   0.162495   0.340314  -0.209472  -0.867176        0.2  -0.301301   \n",
      "2   0.473678   1.280479  -0.094718   0.535878        1.4   1.002168   \n",
      "3  -0.563287  -0.720269   0.793260   0.951145       -0.3  -0.995665   \n",
      "4   0.398579   0.044877   0.560109  -0.541985       -0.9   1.055636   \n",
      "\n",
      "   sensor_10  sensor_11  sensor_12  \n",
      "0  -0.985069   0.531893   4.751492  \n",
      "1   0.082733  -0.231481   0.454390  \n",
      "2   0.449221  -0.586420  -4.736147  \n",
      "3  -0.434290   1.344650   0.429241  \n",
      "4   0.812631   0.123457  -0.223359  \n"
     ]
    }
   ],
   "source": [
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sensor_00', 'sensor_01', 'sensor_02', 'sensor_03', 'sensor_04', 'sensor_05', 'sensor_06', 'sensor_07', 'sensor_08', 'sensor_09', 'sensor_10', 'sensor_11', 'sensor_12']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features = train.columns.tolist()[3:]\n",
    "sc = StandardScaler()\n",
    "train[features] = sc.fit_transform(train[features])\n",
    "test[features] = sc.transform(test[features])\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Window = 60\n",
    "\n",
    "train = train.drop([\"sequence\", \"subject\", \"step\"], axis=1).to_numpy()\n",
    "train = train.reshape(-1, train.shape[-1], Window)\n",
    "train = np.transpose(train, (0, 2, 1))\n",
    "\n",
    "y_train = labels['state'].to_numpy()\n",
    "\n",
    "test = test.drop([\"sequence\", \"subject\", \"step\"], axis=1).to_numpy()\n",
    "test = test.reshape(-1, test.shape[-1], Window)\n",
    "test = np.transpose(test, (0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25968, 60, 13)\n",
      "(25968,)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([0., 1., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "train_X = torch.tensor(train_X, dtype=torch.float32)\n",
    "train_y = torch.tensor(train_y, dtype=torch.float32)\n",
    "print(train_y[0:5])\n",
    "train_tensor = TensorDataset(train_X, train_y)\n",
    "\n",
    "valid_X = torch.tensor(valid_X, dtype=torch.float32)\n",
    "valid_y = torch.tensor(valid_y, dtype=torch.float32) \n",
    "valid_tensor = TensorDataset(valid_X, valid_y)\n",
    "\n",
    "test_X = torch.tensor(test, dtype=torch.float32)\n",
    "test_tensor = TensorDataset(test_X)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_tensor, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_tensor, batch_size=64)\n",
    "test_loader = DataLoader(dataset=test_tensor, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Reshape\n",
      "Shape of training set: (25968, 60, 13)\n",
      "Shape of test set: (12218, 60, 13)\n",
      "Dataloaders are created!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train = pd.read_csv('../data/sensor/train.csv')\n",
    "test = pd.read_csv('../data/sensor/test.csv')\n",
    "train_labels = pd.read_csv(\"../data/sensor/train_labels.csv\")\n",
    "\n",
    "train = train.set_index([\"sequence\", \"subject\", \"step\"])\n",
    "test = test.set_index([\"sequence\", \"subject\", \"step\"])\n",
    "\n",
    "def add_features(df, features):\n",
    "    for feature in features:\n",
    "        df_grouped = df.groupby(\"sequence\")[feature]\n",
    "        df_rolling = df_grouped.rolling(5, center=True)\n",
    "        \n",
    "        df[feature + \"_lag1\"] = df_grouped.shift(1)\n",
    "        df[feature + \"_diff1\"] = df[feature] - df[feature + \"_lag1\"]\n",
    "        df[feature + \"_lag2\"] = df_grouped.shift(2)\n",
    "        df[feature + \"_diff2\"] = df[feature] - df[feature + \"_lag2\"]\n",
    "        df[feature + \"_roll_mean\"] = df_rolling.mean().reset_index(0, drop=True)\n",
    "        df[feature + \"_roll_std\"] = df_rolling.std().reset_index(0, drop=True)\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    return\n",
    "\n",
    "features = [\"sensor_{:02d}\".format(i) for i in range(13)]\n",
    "# add_features(train, features)\n",
    "# add_features(test, features)\n",
    "train.head()\n",
    "\n",
    "input_size = train.shape[1]\n",
    "sequence_length = len(train.index.get_level_values(2).unique())\n",
    "\n",
    "# Scaling test and train\n",
    "scaler = StandardScaler()\n",
    "train = scaler.fit_transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "# Reshaping:\n",
    "train = train.reshape(-1, sequence_length, input_size)\n",
    "test = test.reshape(-1, sequence_length, input_size)\n",
    "print(\"After Reshape\")\n",
    "print(\"Shape of training set: {}\".format(train.shape))\n",
    "print(\"Shape of test set: {}\".format(test.shape))\n",
    "\n",
    "\n",
    "# Splitting train data set into train and validation sets\n",
    "# validation size is selected as 0.2\n",
    "t_X, v_X, t_y, v_y = train_test_split(train, train_labels.state, test_size=0.20,\n",
    "                                      shuffle=True, random_state=0)\n",
    "\n",
    "# Converting train, validation and test data into tensors\n",
    "train_X_tensor = torch.tensor(t_X).float()\n",
    "val_X_tensor = torch.tensor(v_X).float()\n",
    "test_tensor = torch.tensor(test).float()\n",
    "\n",
    "# Converting train and validation labels into tensors\n",
    "train_y_tensor = torch.tensor(t_y.values)\n",
    "val_y_tensor = torch.tensor(v_y.values)\n",
    "\n",
    "# Creating train and validation tensors\n",
    "train_tensor = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "val_tensor = TensorDataset(val_X_tensor, val_y_tensor)\n",
    "\n",
    "# Defining the dataloaders\n",
    "dataloaders = dict()\n",
    "dataloaders[\"train\"] = DataLoader(train_tensor, batch_size=64, shuffle=True)\n",
    "dataloaders[\"val\"] = DataLoader(val_tensor, batch_size=32)\n",
    "dataloaders[\"test\"] = DataLoader(test_tensor, batch_size=32)\n",
    "print(\"Dataloaders are created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, seq_len, dropout=0.5, output_size=1):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # LSTM Layers\n",
    "        self.lstm_1 = nn.LSTM(input_size, hidden_sizes[0], num_layers=2,\n",
    "                            batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.lstm_21 = nn.LSTM(2*hidden_sizes[0], hidden_sizes[1], num_layers=2,\n",
    "                             batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.lstm_22 = nn.LSTM(input_size, hidden_sizes[1], num_layers=2,\n",
    "                             batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.lstm_31 = nn.LSTM(2*hidden_sizes[1], hidden_sizes[2], num_layers=2,\n",
    "                             batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.lstm_32 = nn.LSTM(4*hidden_sizes[1], hidden_sizes[2], num_layers=2,\n",
    "                             batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.lstm_41 = nn.LSTM(2*hidden_sizes[2], hidden_sizes[3], num_layers=2,\n",
    "                             batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.lstm_42 = nn.LSTM(4*hidden_sizes[2], hidden_sizes[3], num_layers=2,\n",
    "                             batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        hidd = 2*hidden_sizes[0] + 4*(hidden_sizes[1]+hidden_sizes[2]+hidden_sizes[3])\n",
    "        self.lstm_5 = nn.LSTM(hidd, hidden_sizes[4], num_layers=2,\n",
    "                             batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc = nn.Sequential(nn.Linear(2*hidden_sizes[4]*seq_len, 4096),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Dropout(p=dropout),\n",
    "                                nn.Linear(4096, 1024),\n",
    "                                nn.ReLU(inplace=True),\n",
    "                                nn.Dropout(p=dropout),\n",
    "                                nn.Linear(1024, output_size),\n",
    "                                nn.Sigmoid()\n",
    "                               )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # lstm layers:\n",
    "        x1, _ = self.lstm_1(x)\n",
    "        \n",
    "        x_x1, _ = self.lstm_21(x1)\n",
    "        x_x2, _ = self.lstm_22(x)\n",
    "        x2 = torch.cat([x_x1, x_x2], dim=2)\n",
    "        \n",
    "        x_x1, _ = self.lstm_31(x_x1)\n",
    "        x_x2, _ = self.lstm_32(x2)\n",
    "        x3 = torch.cat([x_x1, x_x2], dim=2)\n",
    "        \n",
    "        x_x1, _ = self.lstm_41(x_x1)\n",
    "        x_x2, _ = self.lstm_42(x3)\n",
    "        x4 = torch.cat([x_x1, x_x2], dim=2)\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=2)\n",
    "        x, _ = self.lstm_5(x)\n",
    "        \n",
    "        # fully connected layers:\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, num_epochs, criterion, optimizer, train_loader, valid_loader, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader)}\")\n",
    "        # running_loss = 0.0\n",
    "        # with torch.no_grad():\n",
    "        #     correct = 0\n",
    "        #     total = 0\n",
    "        #     for i, data in enumerate(valid_loader, 0):\n",
    "        #         v_inputs, v_labels = data\n",
    "        #         v_inputs = v_inputs.to(device)\n",
    "        #         v_labels = v_labels.to(device).unsqueeze(1).float()\n",
    "        #         outputs = model(v_inputs)\n",
    "        #         loss = criterion(outputs, v_labels)\n",
    "        #         running_loss += loss.item()\n",
    "        #         predicts = torch.round(outputs).squeeze()\n",
    "        #         total += v_labels.size(0)\n",
    "        #         correct += (predicts == v_labels).sum().item()\n",
    "\n",
    "        # print(f\"Epoch {epoch+1}, Validation Loss: {running_loss/len(valid_loader)}\")\n",
    "        # print(f'Validation Accuracy: {100 * correct / total:.2f}%')oss: {running_loss/len(train_loader)}\")\n",
    "        \n",
    "        # model.eval()\n",
    "        # running_loss = 0.0\n",
    "        # with torch.no_grad():\n",
    "        #     correct = 0\n",
    "        #     total = 0\n",
    "        #     for i, data in enumerate(valid_loader, 0):\n",
    "        #         v_inputs, v_labels = data\n",
    "        #         v_inputs = v_inputs.to(device)\n",
    "        #         v_labels = v_labels.to(device).unsqueeze(1).float()\n",
    "        #         outputs = model(v_inputs)\n",
    "        #         loss = criterion(outputs, v_labels)\n",
    "        #         running_loss += loss.item()\n",
    "        #         predicts = torch.round(outputs).squeeze()\n",
    "        #         total += v_labels.size(0)\n",
    "        #         correct += (predicts == v_labels).sum().item()\n",
    "\n",
    "        # print(f\"Epoch {epoch+1}, Validation Loss: {running_loss/len(valid_loader)}\")\n",
    "        # print(f'Validation Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VALIDATION FUNCTION\n",
    "def validation(model, loader, criterion, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    preds_all = torch.LongTensor()\n",
    "    labels_all = torch.LongTensor()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, labels in loader:\n",
    "            labels_all = torch.cat((labels_all, labels), dim=0)\n",
    "            batch_x, labels = batch_x.to(device), labels.to(device)\n",
    "            labels = labels.unsqueeze(1).float()\n",
    "            \n",
    "            output = model.forward(batch_x)\n",
    "            loss += criterion(output,labels).item()\n",
    "            preds_all = torch.cat((preds_all, output.to(\"cpu\")), dim=0)\n",
    "    total_loss = loss/len(loader)\n",
    "    auc_score = roc_auc_score(labels_all, preds_all)\n",
    "    return total_loss, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "0.6936855932382436\n",
      "0.6929685115814209\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(batch_x)\n\u001b[1;32m     44\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[0;32m---> 45\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "num_epochs = 30\n",
    "# learning_rate = 0.01\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# model = TimeSeriesClassifier(\n",
    "#     input_size=13,\n",
    "#     num_classes=1,\n",
    "#     seq_len=60,\n",
    "#     conv_filters=64,\n",
    "#     lstm_units=128,\n",
    "#     dropout_rate=0.2\n",
    "# ).to(device)\n",
    "\n",
    "# model = SimpleRNN(\n",
    "#     input_size=13,\n",
    "#     hidden_size=128,\n",
    "#     seq_len=60\n",
    "# ).to(device)\n",
    "\n",
    "hidden_sizes = [288, 192, 144, 96, 32]\n",
    "max_learning_rate = 0.001\n",
    "\n",
    "model = RNN(\n",
    "    input_size=13,\n",
    "    hidden_sizes=hidden_sizes,\n",
    "    seq_len=60\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=max_learning_rate)\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_x, labels in dataloaders[\"train\"]:\n",
    "        batch_x, labels = batch_x.to(device), labels.to(device)\n",
    "        labels = labels.unsqueeze(1).float()\n",
    "        \n",
    "        # Training \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(batch_x)\n",
    "        loss = criterion(output, labels)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss, train_auc = validation(model, dataloaders[\"train\"], criterion, device)\n",
    "    print(train_loss)\n",
    "    # train_loss = 0.0\n",
    "    # for i, (inputs, labels) in enumerate(dataloaders[\"train\"]):\n",
    "    #     inputs, labels = inputs.to(device), labels.to(device).unsqueeze(1).float()\n",
    "    #     optimizer.zero_grad()\n",
    "    #     outputs = model(inputs)\n",
    "    #     loss = criterion(outputs, labels)\n",
    "    #     train_loss += loss.item()\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "    # print(f\"Epoch {e+1}, Loss: {train_loss / len(dataloaders['train'])}\")\n",
    "\n",
    "# training_loop(model, num_epochs, criterion, optimizer, dataloaders[\"train\"], dataloaders[\"val\"], device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
